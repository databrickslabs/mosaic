{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbe5386-57d1-4594-aa32-e6e52b3ebb97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"xarray[io]==2024.03.0\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a8688ee-29a1-40b7-b5f6-7e5a87042c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dddd90f6-3195-4830-b6e1-4259102ea402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import *\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73f8fc2-3a20-437b-b097-4a70120cb89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_schema = (\n",
    "  StructType()\n",
    "    .add(StructField(\"data_var\", StringType(), False))\n",
    "    .add(StructField(\"year\", IntegerType(), False))\n",
    "    .add(StructField(\"lat\", DoubleType(), False))\n",
    "    .add(StructField(\"lon\", DoubleType(), False))\n",
    ")\n",
    "for d in range(1, 365 + 1):\n",
    "  csv_schema.add(StructField(f\"day_{d}\", DoubleType(), False))\n",
    "\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3818cc-fbdb-4211-9023-e7e993ca7fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_slice(\n",
    "  xds: xr.Dataset, data_var: str, lat_idx: int, lon_start_idx: int = None, lon_end_idx: int = None, save_dir=None\n",
    ") -> str:\n",
    "  \"\"\"\n",
    "  Populate a dict with result information.\n",
    "  Returns either array of json str or the path saved if save_dir provided.\n",
    "  - If lon_start_idx is not provided, then lon slice will start at 0\n",
    "  - If lon_end_idx is not provided, then slice will end at the size of the lon array\n",
    "  \"\"\"\n",
    "  import json\n",
    "  import numpy as np\n",
    "  import os\n",
    "\n",
    "  # retrieve the path\n",
    "  vol_path = xds.encoding['source']\n",
    "\n",
    "  save_path = ''\n",
    "  if save_dir:\n",
    "      os.makedirs(save_dir, exist_ok=True)\n",
    "      if lon_start_idx is not None and lon_end_idx is not None:\n",
    "        save_path = f'{save_dir}/{data_var}_lat_{lat_idx}_lon_{lon_start_idx}-{lon_end_idx}.csv'\n",
    "      elif lon_start_idx is not None:\n",
    "        save_path = f'{save_dir}/{data_var}_lat_{lat_idx}_lon_start_{lon_start_idx}.csv'\n",
    "      elif lon_end_idx is not None:\n",
    "        save_path = f'{save_dir}/{data_var}_lat_{lat_idx}_lon_end_{lon_end_idx}.csv'\n",
    "      else:\n",
    "        save_path = f'{save_dir}/{data_var}_lat_{lat_idx}.csv'\n",
    "      if os.path.exists(save_path):\n",
    "        return [save_path]\n",
    "\n",
    "  # iterate over the lon range\n",
    "  # - ensure end within lon size\n",
    "  start_idx = 0\n",
    "  if lon_start_idx is not None:\n",
    "    start_idx = lon_start_idx\n",
    "  end_idx = xds.lon.size - 1\n",
    "  if lon_end_idx is not None:\n",
    "    end_idx = lon_end_idx\n",
    "  xy_slice = xds.isel(lat=lat_idx, lon=slice(start_idx, end_idx), missing_dims='ignore')\n",
    "\n",
    "  # year for the slice\n",
    "  # - assumes there is only one\n",
    "  year = int(list(set(xy_slice.time.dt.year.values))[0])\n",
    "  \n",
    "  pdf = (\n",
    "    xy_slice\n",
    "      .compute()\n",
    "      .to_dataframe()\n",
    "      .reset_index()\n",
    "  )\n",
    "\n",
    "  day_cols = [f'day_{dy}' for dy in range(1, 365 + 1)]\n",
    "  sel_cols = ['data_var', 'year', 'lat', 'lon'] + day_cols\n",
    "\n",
    "  pdf.rename(columns={data_var: \"value\"}, inplace=True)\n",
    "  pdf['data_var'] = data_var\n",
    "  pdf['year'] = year\n",
    "  pdf['day'] = [f'day_{dy}' for dy in pdf.time.dt.dayofyear]\n",
    "  pdf.drop(columns=['time'], inplace=True)\n",
    "\n",
    "  pdf = pdf.pivot(\n",
    "    index=['data_var', 'year', 'lat', 'lon'],\n",
    "    columns=['day'],\n",
    "    values='value'\n",
    "  ).reset_index()\n",
    "\n",
    "  pdf = pdf[sel_cols]\n",
    "\n",
    "  # save or return dict\n",
    "  if save_dir:\n",
    "    with open(save_path, 'w') as f:\n",
    "      f.write(pdf.to_csv(index=False))\n",
    "    return save_path\n",
    "  else:\n",
    "    d = {}\n",
    "    d['data_var'] = data_var\n",
    "    d['year'] = year\n",
    "    d['lat_idx'] = lat_idx\n",
    "    d['lon_start_idx'] = start_idx\n",
    "    d['lon_end_idx'] = end_idx\n",
    "    d['vol_path'] = vol_path\n",
    "    d['data'] = pdf.to_json(orient='split')\n",
    "    return json.dumps(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026c1c8f-4624-4dd6-96de-728de881ab20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "\n",
    "def process_slice_pd(itr: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "  \"\"\"\n",
    "  Use with `mapInPandas` to add the result column.\n",
    "  \"\"\"\n",
    "  xds = None\n",
    "  curr_vol_path = \"\"\n",
    "  vol_dir_out = \"\"\n",
    "  filename = \"\"\n",
    "\n",
    "  for pdf in itr:\n",
    "    # assume just 1 value for each\n",
    "    result_arr = []\n",
    "    for index, row in pdf.iterrows():\n",
    "      vol_path = row['vol_path']\n",
    "      data_var = row['data_var']\n",
    "      lat_idx = row['lat_idx']\n",
    "      \n",
    "      # unpack the list of results from into their own rows\n",
    "      if xds is None or vol_path != curr_vol_path:\n",
    "        xds= xr.open_dataset(vol_path)\n",
    "        curr_vol_path = vol_path\n",
    "        filename = os.path.basename(vol_path)\n",
    "        vol_dir_out = f\"{row['vol_dir_out']}/{filename}\"\n",
    "      result_arr.append(process_slice(xds, data_var, lat_idx, save_dir=vol_dir_out))\n",
    "    ps = pd.DataFrame([result_arr], columns=['result'])\n",
    "    yield ps.explode('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c77f1d1f-ff98-48cd-996e-341959966ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_msg(msg:str, debug_thresh:int, debug_mode:int):\n",
    "  \"\"\"\n",
    "  Consolidate logging for debug mode.\n",
    "  \"\"\"\n",
    "  if debug_mode >= debug_thresh:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52970d26-8a38-437d-913c-400544a551f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_config_supported():\n",
    "  \"\"\"\n",
    "  Determine if the cluster supports Spark configuration.\n",
    "  - AKA for serverless vs other.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    spark.sparkContext.getConf()\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "print(is_config_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a264ddcb-e3bd-4242-9bd3-60ac1f1536e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_create_table(fqn_tbl:str, drop_table:bool=False, debug_mode:int=0):\n",
    "  \"\"\"\n",
    "  Create table and optionally truncate.\n",
    "  \"\"\"\n",
    "  create_str = \"\"\n",
    "  if drop_table:\n",
    "    log_msg(f\"... create or replace table: {fqn_tbl}\", 1, debug_mode)\n",
    "    create_str += \"create or replace table\"\n",
    "  else:\n",
    "    create_str += \"create table if not exists\"\n",
    "  create_str += f\" {fqn_tbl} (data_var string, year int, lat double, lon double\"\n",
    "  for d in range(1, 365 + 1):\n",
    "    create_str += f\", day_{d} double\"\n",
    "  create_str += \");\"\n",
    "  log_msg(create_str, 2, debug_mode)\n",
    "  sql(create_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc4853e-9821-473c-b9f5-ff5f7f0e2252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_annual(\n",
    "  vol_path:str, data_var:str, fqn_tbl:str, vol_dir_out:str, \n",
    "  drop_table:bool=False, skip_table:bool=False, batch_limit:int=0, debug_mode:bool=0\n",
    ") -> DataFrame:\n",
    "  \"\"\"\n",
    "  Process a netcdf containing annual data. Write to table with columns for each day of the year.\n",
    "  - turning off AQE to help with repartitioning.\n",
    "  - handles batching to avoid memory issues.\n",
    "  - handles create table, can `drop_table` a previous table as well.\n",
    "  - if `skip_table` is True, does not create table.\n",
    "  Returns a DataFrame of the table if `skip_table` is False.\n",
    "  \"\"\"\n",
    "  import xarray as xr\n",
    "\n",
    "  # [1] get initial metadata\n",
    "  # - need lat / lon sizes\n",
    "  xds = xr.open_dataset(vol_path)\n",
    "  n_lat = xds.sizes['lat']\n",
    "  meta_rows = list(range(n_lat))\n",
    "  log_msg(f\"... n_lat: {n_lat:,}\", 1, debug_mode)\n",
    "\n",
    "  # [2] estimate rows\n",
    "  # - including batch_limit\n",
    "  if batch_limit:\n",
    "    log_msg(f\"... limit provided -> # batches? {batch_limit}\", 1, debug_mode)\n",
    "    meta_rows = meta_rows[:batch_limit]\n",
    "  n_part = len(meta_rows)\n",
    "  log_msg(f\"... total batches estimated: {n_part:,}\", 1, debug_mode)\n",
    "\n",
    "  # [3] write batches to file\n",
    "  try:\n",
    "    if is_config_supported():\n",
    "      spark.conf.set(\"spark.databricks.optimizer.adaptive.enabled\", False)   # <- no coalescing\n",
    "      spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", str(1)) # <- 1 row is a batch\n",
    "\n",
    "    df_meta = (\n",
    "      spark\n",
    "        .createDataFrame([(m,) for m in meta_rows], [\"lat_idx\"])\n",
    "          .withColumn(\"data_var\", F.lit(data_var))\n",
    "          .withColumn(\"vol_path\", F.lit(vol_path))\n",
    "          .withColumn(\"vol_dir_out\", F.lit(vol_dir_out))\n",
    "    )\n",
    "    if debug_mode > 1:\n",
    "      df_meta.show(3)\n",
    "\n",
    "    (\n",
    "      df_meta\n",
    "        .repartition(n_part, F.rand())\n",
    "        .mapInPandas(\n",
    "          process_slice_pd,\n",
    "          \"result string\"\n",
    "        )\n",
    "        .write\n",
    "          .format(\"noop\")\n",
    "          .mode(\"overwrite\")\n",
    "        .save()   \n",
    "    )\n",
    "    log_msg(f\"... finished batch(es) per lat\", 1, debug_mode)\n",
    "    \n",
    "    # [4] write files to delta lake\n",
    "    # - todo filter out already saved files\n",
    "    if not skip_table:\n",
    "      ensure_create_table(fqn_tbl, drop_table=drop_table, debug_mode=debug_mode)\n",
    "      filename = os.path.basename(vol_path)\n",
    "      (\n",
    "        spark\n",
    "          .read\n",
    "            .csv(f\"{vol_dir_out}/{filename}/*.csv\", header=True, schema=csv_schema)\n",
    "            .na.fill(0.0)\n",
    "          .write\n",
    "            .mode(\"append\")\n",
    "          .saveAsTable(fqn_tbl)\n",
    "      )\n",
    "    else:\n",
    "      log_msg(f\"... skip_table provided\", 1, debug_mode)\n",
    "  finally:\n",
    "    if is_config_supported():\n",
    "      # back to defaults\n",
    "      spark.conf.set(\"spark.databricks.optimizer.adaptive.enabled\", True)\n",
    "      spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", str(10_000))\n",
    "  if not skip_table:\n",
    "    result_df = spark.table(fqn_tbl)\n",
    "    log_msg(f\"... rows in table? {result_df.count():,}\", 1, debug_mode)\n",
    "    debug_mode > 0 and result_df.limit(5).display()\n",
    "    return result_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "netcdf_helpers.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}